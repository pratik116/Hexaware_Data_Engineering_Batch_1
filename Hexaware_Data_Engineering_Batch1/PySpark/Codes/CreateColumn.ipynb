{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spark.read.csv(r\"DEPT_DATA.CSV\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|dept_name\"|dept_id|\n",
      "+----------+-------+\n",
      "|   Finance|     10|\n",
      "| Marketing|     20|\n",
      "|     Sales|     30|\n",
      "|        IT|     40|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+\n",
      "|dept_name\"|dept_id|Total_Staff|\n",
      "+----------+-------+-----------+\n",
      "|   Finance|     10|         10|\n",
      "| Marketing|     20|         10|\n",
      "|     Sales|     30|         10|\n",
      "|        IT|     40|         10|\n",
      "+----------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "data.withColumn('Total_Staff',lit(10)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+\n",
      "|dept_name\"|dept_id|Total_Staff|\n",
      "+----------+-------+-----------+\n",
      "|   Finance|     10|         50|\n",
      "| Marketing|     20|        100|\n",
      "|     Sales|     30|        150|\n",
      "|        IT|     40|        200|\n",
      "+----------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('Total_Staff',data.dept_id*5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+\n",
      "|dept_name\"|dept_id|     Details|\n",
      "+----------+-------+------------+\n",
      "|   Finance|     10|  10-Finance|\n",
      "| Marketing|     20|20-Marketing|\n",
      "|     Sales|     30|    30-Sales|\n",
      "|        IT|     40|       40-IT|\n",
      "+----------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn('Details',concat_ws('-','dept_id','dept_name\"')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|Text_Data                                                       |\n",
      "+----------------------------------------------------------------+\n",
      "|[Hii!!, My, name, is, pratik, arun, wani., I, am, IT, Engineer.]|\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading Text file\n",
    "df1 = spark.read.text(\"sample.txt\") \n",
    "\n",
    "df1.selectExpr(\"split(value, ' ') as Text_Data\").show(4,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|Text_Data             |\n",
      "+----------------------+\n",
      "|[dept_name\",\"dept_id\"]|\n",
      "|[\"Finance\",10]        |\n",
      "|[\"Marketing\",20]      |\n",
      "|[\"Sales\",30]          |\n",
      "+----------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading CSV file\n",
    "df2 = spark.read.text(\"DEPT_DATA.CSV\") \n",
    "\n",
    "df2.selectExpr(\"split(value, ' ') as Text_Data\").show(4,False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
